#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«
ç”¨äºæŠ“å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹å¹¶é‡æ–°æ’ç‰ˆåˆ°ç½‘ç«™
"""

import requests
import json
import re
import time
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
import html
import os
import hashlib

class WeChatArticleCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # åˆ›å»ºå›¾ç‰‡ç›®å½•
        self.images_dir = 'images'
        if not os.path.exists(self.images_dir):
            os.makedirs(self.images_dir)
    
    def extract_article_id(self, url):
        """ä»å¾®ä¿¡å…¬ä¼—å·URLä¸­æå–æ–‡ç« ID"""
        try:
            parsed = urlparse(url)
            if 'mp.weixin.qq.com' in parsed.netloc:
                # æå–URLä¸­çš„æ–‡ç« ID
                path_parts = parsed.path.split('/')
                if len(path_parts) >= 2:
                    return path_parts[-1]
            return None
        except Exception as e:
            print(f"æå–æ–‡ç« IDå¤±è´¥: {e}")
            return None
    
    def fetch_article_content(self, url):
        """æŠ“å–æ–‡ç« å†…å®¹"""
        try:
            print(f"æ­£åœ¨æŠ“å–æ–‡ç« : {url}")
            
            # å°è¯•ç›´æ¥è®¿é—®
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢
            if "ç¯å¢ƒå¼‚å¸¸" in response.text or "å®ŒæˆéªŒè¯" in response.text:
                print("âš ï¸  æ–‡ç« éœ€è¦éªŒè¯ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
                return self.fetch_with_alternative_method(url)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ–‡ç« ä¿¡æ¯
            article_data = self.parse_article_content(soup, url)
            return article_data
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return None
    
    def fetch_with_alternative_method(self, url):
        """å¤‡ç”¨æŠ“å–æ–¹æ³•"""
        try:
            # å°è¯•ä½¿ç”¨ä¸åŒçš„User-Agent
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9',
            }
            
            response = self.session.get(url, headers=headers, timeout=30)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å¦‚æœè¿˜æ˜¯éœ€è¦éªŒè¯ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            if "ç¯å¢ƒå¼‚å¸¸" in response.text:
                print("âš ï¸  æ–‡ç« éœ€è¦éªŒè¯ï¼Œç”Ÿæˆæ¨¡æ‹Ÿå†…å®¹...")
                return self.generate_mock_content(url)
            
            return self.parse_article_content(soup, url)
            
        except Exception as e:
            print(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            return self.generate_mock_content(url)
    
    def generate_mock_content(self, url):
        """ç”Ÿæˆæ¨¡æ‹Ÿå†…å®¹ï¼ˆå½“æ— æ³•æŠ“å–æ—¶ï¼‰"""
        article_id = self.extract_article_id(url) or "unknown"
        
        return {
            'id': f'wechat-{article_id}',
            'title': 'å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ',
            'source': 'å¾®ä¿¡å…¬ä¼—å·',
            'summary': 'è¿™æ˜¯ä¸€ç¯‡æ¥è‡ªå¾®ä¿¡å…¬ä¼—å·çš„æ–‡ç« ï¼Œç”±äºè®¿é—®é™åˆ¶ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹ã€‚è¯·ç‚¹å‡»"é˜…è¯»åŸæ–‡"æŸ¥çœ‹å®Œæ•´å†…å®¹ã€‚',
            'url': url,
            'date': datetime.now().strftime('%Y-%m-%d'),
            'tags': ['å¾®ä¿¡å…¬ä¼—å·', 'AIæŠ€æœ¯'],
            'content': '''
                <h2>å¾®ä¿¡å…¬ä¼—å·æ–‡ç« </h2>
                <p>ç”±äºå¾®ä¿¡å…¬ä¼—å·çš„è®¿é—®é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è·å–è¿™ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹ã€‚</p>
                <p>è¯·ç‚¹å‡»ä¸‹æ–¹çš„"é˜…è¯»åŸæ–‡"æŒ‰é’®ï¼Œåœ¨æ–°çª—å£ä¸­æŸ¥çœ‹å®Œæ•´çš„æ–‡ç« å†…å®¹ã€‚</p>
                <blockquote>
                    <p><strong>æç¤ºï¼š</strong>å¦‚æœé‡åˆ°"ç¯å¢ƒå¼‚å¸¸"æç¤ºï¼Œè¯·æŒ‰ç…§é¡µé¢æŒ‡å¼•å®ŒæˆéªŒè¯åå³å¯æ­£å¸¸è®¿é—®ã€‚</p>
                </blockquote>
                <p>æˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ”¹è¿›æŠ“å–æŠ€æœ¯ï¼Œä»¥ä¾¿ä¸ºæ‚¨æä¾›æ›´å¥½çš„é˜…è¯»ä½“éªŒã€‚</p>
            '''
        }
    
    def parse_article_content(self, soup, url):
        """è§£ææ–‡ç« å†…å®¹"""
        try:
            # ç”Ÿæˆæ–‡ç« ID
            article_id = self.extract_article_id(url) or f"article-{int(time.time())}"
            
            # æå–æ ‡é¢˜
            title = self.extract_title(soup)
            
            # æå–ä½œè€…/æ¥æº
            source = self.extract_source(soup)
            
            # æå–æ­£æ–‡å†…å®¹
            content = self.extract_content(soup)
            
            # å¤„ç†å›¾ç‰‡ä¸‹è½½
            content = self.process_article_images(content, article_id)
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self.generate_summary(content)
            
            # æå–æ ‡ç­¾
            tags = self.extract_tags(content, title)
            
            return {
                'id': f'wechat-{article_id}',
                'title': title,
                'source': source,
                'summary': summary,
                'url': url,
                'date': datetime.now().strftime('%Y-%m-%d'),
                'tags': tags,
                'content': content
            }
            
        except Exception as e:
            print(f"âŒ è§£æå†…å®¹å¤±è´¥: {e}")
            return None
    
    def extract_title(self, soup):
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        title_selectors = [
            'h1#activity-name',
            'h1.rich_media_title',
            'h1',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                if title and len(title) > 3:
                    return title
        
        return "å¾®ä¿¡å…¬ä¼—å·æ–‡ç« "
    
    def extract_source(self, soup):
        """æå–æ–‡ç« æ¥æº"""
        # å°è¯•æå–å…¬ä¼—å·åç§°
        source_selectors = [
            '.profile_nickname',
            '.rich_media_meta_text',
            '.author'
        ]
        
        for selector in source_selectors:
            source_elem = soup.select_one(selector)
            if source_elem:
                source = source_elem.get_text().strip()
                if source:
                    return source
        
        return "å¾®ä¿¡å…¬ä¼—å·"
    
    def extract_content(self, soup):
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            '#js_content',
            '.rich_media_content',
            '.content',
            'article'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†å†…å®¹
                content = self.clean_content(content_elem)
                if content and len(content) > 100:
                    return content
        
        return "<p>æ— æ³•æå–æ–‡ç« å†…å®¹ï¼Œè¯·ç‚¹å‡»åŸæ–‡é“¾æ¥æŸ¥çœ‹ã€‚</p>"
    
    def clean_content(self, content_elem):
        """æ¸…ç†å’Œé‡æ–°æ ¼å¼åŒ–å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for elem in content_elem.find_all(['script', 'style', 'noscript']):
            elem.decompose()
        
        # ç§»é™¤éšè—æ ·å¼
        for elem in content_elem.find_all(attrs={'style': True}):
            style = elem.get('style', '')
            # ç§»é™¤visibility: hiddenå’Œopacity: 0ç­‰éšè—æ ·å¼
            if 'visibility: hidden' in style or 'opacity: 0' in style:
                # ä¿ç•™å…¶ä»–æ ·å¼ï¼Œåªç§»é™¤éšè—ç›¸å…³çš„
                new_style = re.sub(r'visibility\s*:\s*hidden[^;]*;?', '', style)
                new_style = re.sub(r'opacity\s*:\s*0[^;]*;?', '', style)
                new_style = re.sub(r';\s*;', ';', new_style)  # æ¸…ç†å¤šä½™çš„åˆ†å·
                new_style = new_style.strip('; ')
                if new_style:
                    elem['style'] = new_style
                else:
                    del elem['style']
        
        # å¤„ç†å›¾ç‰‡
        for img in content_elem.find_all('img'):
            src = img.get('data-src') or img.get('src')
            if src:
                img['src'] = src
                img['alt'] = img.get('alt', 'å›¾ç‰‡')
                # æ·»åŠ å“åº”å¼æ ·å¼
                img['style'] = 'max-width: 100%; height: auto; display: block; margin: 16px auto;'
        
        # å¤„ç†é“¾æ¥
        for link in content_elem.find_all('a'):
            href = link.get('href')
            if href and not href.startswith('http'):
                link['href'] = f"https://mp.weixin.qq.com{href}"
            link['target'] = '_blank'
            link['rel'] = 'noopener'
        
        # è½¬æ¢ä¸ºHTMLå­—ç¬¦ä¸²
        content_html = str(content_elem)
        
        # æ¸…ç†HTML
        content_html = re.sub(r'\s+', ' ', content_html)
        content_html = re.sub(r'>\s+<', '><', content_html)
        
        return content_html
    
    def generate_summary(self, content):
        """ç”Ÿæˆæ–‡ç« æ‘˜è¦"""
        try:
            # ç§»é™¤HTMLæ ‡ç­¾
            text_content = re.sub(r'<[^>]+>', '', content)
            text_content = re.sub(r'\s+', ' ', text_content).strip()
            
            # å–å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
            if len(text_content) > 200:
                summary = text_content[:200] + "..."
            else:
                summary = text_content
            
            return summary
        except:
            return "è¿™æ˜¯ä¸€ç¯‡æ¥è‡ªå¾®ä¿¡å…¬ä¼—å·çš„æ–‡ç« ï¼Œå†…å®¹ç²¾å½©ï¼Œå€¼å¾—ä¸€è¯»ã€‚"
    
    def extract_tags(self, content, title):
        """æå–æ–‡ç« æ ‡ç­¾"""
        tags = []
        
        # ä»æ ‡é¢˜å’Œå†…å®¹ä¸­æå–å…³é”®è¯
        text = f"{title} {content}".lower()
        
        # AIç›¸å…³å…³é”®è¯
        ai_keywords = ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ¨¡å‹', 'chatgpt', 'gpt', 'llm', 'ç¥ç»ç½‘ç»œ']
        tech_keywords = ['æŠ€æœ¯', 'ç®—æ³•', 'ç¼–ç¨‹', 'å¼€å‘', 'ä»£ç ', 'è½¯ä»¶', 'ç³»ç»Ÿ', 'å¹³å°']
        trend_keywords = ['è¶‹åŠ¿', 'å‘å±•', 'æœªæ¥', 'åˆ›æ–°', 'çªç ´', 'åº”ç”¨', 'å®è·µ']
        
        for keyword in ai_keywords + tech_keywords + trend_keywords:
            if keyword in text:
                tags.append(keyword)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        tags = list(set(tags))[:5]
        
        if not tags:
            tags = ['æŠ€æœ¯', 'AI', 'æ–‡ç« ']
        
        return tags
    
    def download_image(self, image_url, article_id):
        """ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(image_url)
            filename = os.path.basename(parsed_url.path)
            
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶åï¼Œä½¿ç”¨URLçš„hash
            if not filename or '.' not in filename:
                url_hash = hashlib.md5(image_url.encode()).hexdigest()
                filename = f"{url_hash}.jpg"
            
            # æ·»åŠ æ–‡ç« IDå‰ç¼€é¿å…å†²çª
            filename = f"{article_id}_{filename}"
            local_path = os.path.join(self.images_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
            if os.path.exists(local_path):
                return f"./{self.images_dir}/{filename}"
            
            # ä¸‹è½½å›¾ç‰‡
            print(f"æ­£åœ¨ä¸‹è½½å›¾ç‰‡: {image_url}")
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜å›¾ç‰‡
            with open(local_path, 'wb') as f:
                f.write(response.content)
            
            print(f"å›¾ç‰‡å·²ä¿å­˜: {local_path}")
            return f"./{self.images_dir}/{filename}"
            
        except Exception as e:
            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
            return image_url  # è¿”å›åŸURLä½œä¸ºå¤‡ç”¨
    
    def process_article_images(self, article_content, article_id):
        """å¤„ç†æ–‡ç« ä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
        if not article_content:
            return article_content
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        img_matches = re.findall(img_pattern, article_content)
        
        for img_url in img_matches:
            if 'mmbiz.qpic.cn' in img_url or 'res.wx.qq.com' in img_url:
                # ä¸‹è½½å›¾ç‰‡
                local_path = self.download_image(img_url, article_id)
                
                # æ›¿æ¢URL
                article_content = article_content.replace(img_url, local_path)
        
        return article_content
    
    def update_articles_json(self, new_article):
        """æ›´æ–°articles.jsonæ–‡ä»¶"""
        try:
            json_file = 'posts/articles.json'
            
            # è¯»å–ç°æœ‰æ–‡ç« 
            if os.path.exists(json_file):
                with open(json_file, 'r', encoding='utf-8') as f:
                    articles = json.load(f)
            else:
                articles = []
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒIDçš„æ–‡ç« 
            existing_ids = [article.get('id') for article in articles]
            if new_article['id'] in existing_ids:
                # æ›´æ–°ç°æœ‰æ–‡ç« 
                for i, article in enumerate(articles):
                    if article.get('id') == new_article['id']:
                        articles[i] = new_article
                        print(f"âœ… æ›´æ–°äº†ç°æœ‰æ–‡ç« : {new_article['title']}")
                        break
            else:
                # æ·»åŠ æ–°æ–‡ç« åˆ°å¼€å¤´
                articles.insert(0, new_article)
                print(f"âœ… æ·»åŠ äº†æ–°æ–‡ç« : {new_article['title']}")
            
            # ä¿å­˜æ–‡ä»¶
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“ å·²æ›´æ–° {json_file}")
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–°JSONæ–‡ä»¶å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çˆ¬è™«å¯åŠ¨")
    print("=" * 50)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    url = input("è¯·è¾“å…¥å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥: ").strip()
    
    if not url:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„URL")
        return
    
    if 'mp.weixin.qq.com' not in url:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WeChatArticleCrawler()
    
    # æŠ“å–æ–‡ç« 
    article_data = crawler.fetch_article_content(url)
    
    if article_data:
        print("\nğŸ“„ æ–‡ç« ä¿¡æ¯:")
        print(f"æ ‡é¢˜: {article_data['title']}")
        print(f"æ¥æº: {article_data['source']}")
        print(f"æ—¥æœŸ: {article_data['date']}")
        print(f"æ ‡ç­¾: {', '.join(article_data['tags'])}")
        print(f"æ‘˜è¦: {article_data['summary'][:100]}...")
        
        # è¯¢é—®æ˜¯å¦æ›´æ–°åˆ°ç½‘ç«™
        confirm = input("\næ˜¯å¦å°†æ­¤æ–‡ç« æ·»åŠ åˆ°ç½‘ç«™? (y/n): ").strip().lower()
        
        if confirm in ['y', 'yes', 'æ˜¯', 'ç¡®å®š']:
            if crawler.update_articles_json(article_data):
                print("\nğŸ‰ æ–‡ç« å·²æˆåŠŸæ·»åŠ åˆ°ç½‘ç«™!")
                print("è¯·åˆ·æ–°æµè§ˆå™¨æŸ¥çœ‹æ›´æ–°åçš„å†…å®¹ã€‚")
            else:
                print("\nâŒ æ·»åŠ æ–‡ç« å¤±è´¥")
        else:
            print("\nâŒ å·²å–æ¶ˆæ·»åŠ ")
    else:
        print("\nâŒ æŠ“å–æ–‡ç« å¤±è´¥")

if __name__ == "__main__":
    main()
